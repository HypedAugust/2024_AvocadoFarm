{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my task is to fill in this py. \n",
    "# worker is a part of chatbot app that processes user msgs and documents \n",
    "# worker uses langchain library to handle pdf and llms \n",
    "\n",
    "# placeholder for Watsonx_API and Project_id incase you need to use the code outside this environment\n",
    "Watsonx_API = \"\"\n",
    "Project_id= \"skills-network\"\n",
    "\n",
    "# initialization \n",
    "# Function to initialize the language model and its embeddings\n",
    "def init_llm():\n",
    "    global llm_hub, embeddings\n",
    "    my_credentials = {\n",
    "        \"url\"    : \"https://us-south.ml.cloud.ibm.com\"\n",
    "    } # 질문 : my credentials을 하는 이유가 뭐야? \n",
    "    params = {\n",
    "            GenParams.MAX_NEW_TOKENS: 500, # The maximum number of tokens that the model can generate in a single run.\n",
    "            GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n",
    "        }\n",
    "    LLAMA2_model = Model(\n",
    "            model_id= 'meta-llama/llama-3-8b-instruct', \n",
    "            credentials=my_credentials,\n",
    "            params=params,\n",
    "            project_id=\"skills-network\",  # <--- NOTE: specify \"skills-network\" as your project_id\n",
    "            )\n",
    "    llm_hub = WatsonxLLM(model=LLAMA2_model)\n",
    "\n",
    "#Initialize embeddings\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={\"device\": DEVICE}\n",
    "    )\n",
    "# 질문 : kwargs은 무슨뜻이야? \n",
    "\n",
    "# Document processing \n",
    "def process_document(document_path):\n",
    "    global conversation_retrieval_chain \n",
    "\n",
    "    # document loading \n",
    "    loader = PyPDFLoader(document_path)\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    # document splitting \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # vector store creation \n",
    "    db = Chroma.from_documents(texts, embedding=embeddings) \n",
    "\n",
    "    # QA chain that uses LLM and retriever for QA # 질문 : 이 코드에서 정확히 QAchain의 용도를 모르겠어. \n",
    "    conversation_retrieval_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm_hub,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever= db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6, 'lambda_mult': 0.25}),\n",
    "        # 질문 : retriever code를 이해하기 어려워 쉽게 설명해줘. \n",
    "        return_source_documents=False\n",
    "    )\n",
    "\n",
    "\n",
    "# user prompt processing \n",
    "def process_prompt(prompt):\n",
    "    global conversation_retrieval_chain\n",
    "    global chat_history \n",
    "\n",
    "    # Pass the prompt and the chat history to the conversation_retrieval_chain object\n",
    "    output = conversation_retrieval_chain({\"question\": prompt, \"chat_history\": chat_history})\n",
    "    answer =  output[\"result\"]\n",
    "\n",
    "    # update the chat history \n",
    "    chat_history.append((prompt, answer)) # 질문 : udpate하는 용도를 알려줘 . \n",
    "\n",
    "    # return model's answer \n",
    "    return answer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
