{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([2, 2]), dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 텐서 생성\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# 질문: x의 shape과 dtype을 출력하는 코드를 작성하세요.\n",
    "# 답변:\n",
    "shape = x.shape  # 텐서의 형태(차원)를 가져옵니다.\n",
    "dtype = x.dtype  # 텐서의 데이터 타입을 가져옵니다.\n",
    "print(f\"shape: {shape}, dtype: {dtype}\")\n",
    "\n",
    "# 해설:\n",
    "# - x.shape는 텐서의 차원을 튜플로 반환합니다. 여기서는 2x2 행렬이므로 (2, 2)가 됩니다.\n",
    "# - x.dtype은 텐서의 데이터 타입을 반환합니다. 정수로 생성했으므로 torch.int64가 됩니다.\n",
    "# - f-string을 사용하여 결과를 포맷팅하여 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 그래프 생성\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(1, 2), (2, 3), (3, 4)])\n",
    "\n",
    "# 질문: 노드 2의 이웃 노드들을 리스트로 반환하는 함수를 작성하세요.\n",
    "# 답변:\n",
    "def get_neighbors(graph, node):\n",
    "    return list(graph.neighbors(node))\n",
    "\n",
    "print(get_neighbors(G, 2))\n",
    "\n",
    "# 해설:\n",
    "# - graph.neighbors(node)는 주어진 노드의 이웃을 반환하는 이터레이터입니다.\n",
    "# - list()를 사용하여 이터레이터를 리스트로 변환합니다.\n",
    "# - 노드 2의 이웃은 1과 3이므로, [1, 3]이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 5)  # 입력 크기 10, 출력 크기 5인 완전연결층\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # 입력 x에 대해 선형 변환 수행\n",
    "\n",
    "# 질문: 입력 크기가 (3, 10)인 텐서를 만들고, SimpleNet의 forward를 호출한 결과의 shape를 출력하세요.\n",
    "# 답변:\n",
    "model = SimpleNet()  # SimpleNet 인스턴스 생성\n",
    "input_tensor = torch.randn(3, 10)  # 크기가 (3, 10)인 랜덤 텐서 생성\n",
    "output = model(input_tensor)  # 모델의 forward 메서드 호출\n",
    "print(output.shape)  # 출력 텐서의 shape 출력\n",
    "\n",
    "# 해설:\n",
    "# - torch.randn(3, 10)은 평균 0, 표준편차 1의 정규분포에서 3x10 크기의 랜덤 텐서를 생성합니다.\n",
    "# - model(input_tensor)는 model.forward(input_tensor)와 동일합니다.\n",
    "# - 출력의 shape는 (3, 5)가 됩니다. 배치 크기 3은 유지되고, fc 층의 출력 크기가 5이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 정의\n",
    "model = SimpleNet()\n",
    "criterion = nn.MSELoss()  # 평균 제곱 오차 손실 함수\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 학습률 0.01의 SGD 옵티마이저\n",
    "\n",
    "# 질문: 입력 x와 목표 y가 주어졌을 때, 역전파를 수행하는 코드를 작성하세요.\n",
    "# 답변:\n",
    "x = torch.randn(3, 10)  # 입력 데이터\n",
    "y = torch.randn(3, 5)   # 목표 출력\n",
    "optimizer.zero_grad()   # 그래디언트 초기화\n",
    "output = model(x)       # 순방향 전파\n",
    "loss = criterion(output, y)  # 손실 계산\n",
    "loss.backward()         # 역전파 수행\n",
    "optimizer.step()        # 파라미터 업데이트\n",
    "\n",
    "# 해설:\n",
    "# - optimizer.zero_grad()는 이전 반복에서 계산된 그래디언트를 초기화합니다.\n",
    "# - loss.backward()는 손실에 대한 그래디언트를 계산합니다.\n",
    "# - optimizer.step()은 계산된 그래디언트를 사용하여 모델의 파라미터를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문: 위의 역전파 예제에서 사용된 옵티마이저(SGD)를 Adam으로 변경하는 코드를 작성하세요.\n",
    "# 답변:\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 해설:\n",
    "# - optim.Adam()은 Adam 옵티마이저를 생성합니다.\n",
    "# - model.parameters()는 모델의 학습 가능한 파라미터를 반환합니다.\n",
    "# - lr=0.01은 학습률을 0.01로 설정합니다.\n",
    "# - Adam은 적응적 학습률을 사용하는 최적화 알고리즘으로, 일반적으로 SGD보다 빠르게 수렴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문: SimpleNet 클래스를 수정하여 Linear 층 다음에 ReLU 활성화 함수를 추가하세요.\n",
    "# 답변:\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 5)  # 완전연결층\n",
    "        self.relu = nn.ReLU()       # ReLU 활성화 함수\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc(x))  # 선형 변환 후 ReLU 적용\n",
    "\n",
    "# 해설:\n",
    "# - nn.ReLU()는 ReLU 활성화 함수 층을 생성합니다.\n",
    "# - forward 메서드에서 self.fc(x)의 결과에 self.relu를 적용합니다.\n",
    "# - ReLU는 음수 입력을 0으로, 양수 입력은 그대로 출력하여 비선형성을 도입합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문: 이진 분류 문제를 위한 BCE(Binary Cross Entropy) 손실 함수를 정의하는 코드를 작성하세요.\n",
    "# 답변:\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 해설:\n",
    "# - nn.BCEWithLogitsLoss()는 이진 교차 엔트로피 손실과 시그모이드 함수를 결합한 손실 함수입니다.\n",
    "# - 이 함수는 수치적으로 더 안정적이며, 별도의 시그모이드 층을 추가할 필요가 없습니다.\n",
    "# - 이진 분류 문제에서 모델의 출력과 실제 레이블(0 또는 1) 간의 차이를 측정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 질문: 크기가 (1000, 10)인 입력 데이터 X와 (1000,)인 레이블 y로 DataLoader를 생성하는 코드를 작성하세요. 배치 크기는 32로 설정하세요.\n",
    "# 답변:\n",
    "X = torch.randn(1000, 10)  # 입력 데이터 생성\n",
    "y = torch.randint(0, 2, (1000,))  # 이진 레이블 생성\n",
    "dataset = TensorDataset(X, y)  # 데이터셋 생성\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # DataLoader 생성\n",
    "\n",
    "# 해설:\n",
    "# - torch.randn(1000, 10)은 1000개의 샘플, 각 10개의 특성을 가진 입력 데이터를 생성합니다.\n",
    "# - torch.randint(0, 2, (1000,))는 0과 1 사이의 1000개의 무작위 정수를 생성합니다.\n",
    "# - TensorDataset은 입력과 레이블을 페어로 묶어 데이터셋을 만듭니다.\n",
    "# - DataLoader는 데이터셋을 미니배치로 나누고, shuffle=True로 에폭마다 데이터를 섞습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문: 'Layer' 기본 클래스를 상속받아 'Dense' 클래스를 구현하세요. forward 메서드를 포함해야 합니다.\n",
    "# 답변:\n",
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weights = torch.randn(in_features, out_features)  # 가중치 초기화\n",
    "        self.bias = torch.zeros(out_features)  # 편향 초기화\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.mm(x, self.weights) + self.bias  # 선형 변환 수행\n",
    "\n",
    "# 해설:\n",
    "# - Layer 클래스는 기본 forward 메서드를 정의합니다.\n",
    "# - Dense 클래스는 Layer를 상속받아 완전연결층을 구현합니다.\n",
    "# - __init__ 메서드에서 가중치와 편향을 초기화합니다.\n",
    "# - forward 메서드는 입력 x에 대해 Wx + b 형태의 선형 변환을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 질문: 입력 x에 대해 순서대로 선형 변환, ReLU 활성화, Dropout을 적용하는 함수를 작성하세요.\n",
    "# 답변:\n",
    "def functional_layer(x, weight, bias, p=0.5):\n",
    "    x = F.linear(x, weight, bias)  # 선형 변환\n",
    "    x = F.relu(x)  # ReLU 활성화\n",
    "    x = F.dropout(x, p=p, training=True)  # Dropout 적용\n",
    "    return x\n",
    "\n",
    "# 해설:\n",
    "# - F.linear(x, weight, bias)는 입력 x에 대해 가중치와 편향을 사용하여 선형 변환을 수행합니다.\n",
    "# - F.relu(x)는 ReLU 활성화 함수를 적용합니다.\n",
    "# - F.dropout(x, p=p, training=True)는 훈련 중 p의 확률로 뉴런을 드롭아웃합니다.\n",
    "# - 이 함수는 순수 함수형 스타일로, 상태를 변경하지 않고 입력에 대한 변환만을 수행합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
