{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter07 Activation Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Activation Function?    \n",
    "\n",
    "1) Non-Linear Activation Function은 Complex Decision Boundary를 학습하기 위해 필요.     \n",
    "\n",
    "2) Non Linearity 덕에 Feature space를 왜곡 시켜서 Non linear한 decision boundary  그려냄.   \n",
    "\n",
    "3) Non Linearity 덕에 feature space의 각 grid는 non uniform 하게 변형됨.    \n",
    "\n",
    "4) Linear는 아무리 Layer 쌓아도 Single NN임.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function 종류 \n",
    "1) Sigmoid   \n",
    "2) Tanh  \n",
    "3) Relu  - CNN\n",
    "4) Leaky ReLU - CNN  \n",
    "5) ELU  - CNN\n",
    "6) SoftMax  - 마지막 Classification Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function(활성화 함수)은 뉴런이 얼마나 강하게 반응할지 결정하는 것임.  \n",
    "Non-linear(비선형) Activation Function을 사용하면 데이터의 복잡한 패턴을 학습할 수 있음.  \n",
    "선형적인 활성화 함수를 사용하면, 복잡한 구조를 만들지 못해서 깊은 신경망의 효과가 사라짐.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gradient Saturation(기울기 포화):     \n",
    "\n",
    "설명: Gradient Saturation은 Activation Function(활성화 함수) 때문에 기울기(Gradient)가 거의 0에 가까워지는 상황을 말함. 쉽게 말해, 신경망이 학습할 때 기울기가 너무 작아져서 더 이상 잘 학습하지 못하는 상태임. 마치 물이 꽉 찬 스펀지가 더 이상 물을 흡수하지 못하는 것과 비슷함.    \n",
    "\n",
    "2. SGD 학습(Stochastic Gradient Descent):     \n",
    "\n",
    "설명: Stochastic Gradient Descent는 컴퓨터가 데이터를 조금씩 보면서 학습하는 방법임. 데이터를 한꺼번에 다 보지 않고, 작은 덩어리씩 나누어 처리하면서, 그때마다 모델의 가중치(W)를 조금씩 조정하는 방식임. 이를 통해 컴퓨터는 효율적으로 학습할 수 있음.   \n",
    "\n",
    "3. Dead Neuron(죽은 뉴런):    \n",
    "\n",
    "설명: Dead Neuron은 뉴런이 더 이상 반응하지 않게 된 상태를 의미함. 뉴런이 항상 0이나 비슷한 값을 출력해서 학습에 도움이 되지 않는 상태를 말하는데, 마치 고장 난 전등처럼 아무리 신호를 보내도 반응이 없는 뉴런임.\n",
    " \n",
    "4. Decision Boundary(결정 경계):    \n",
    "\n",
    "설명: Decision Boundary는 데이터를 분류하는 경계를 의미함. 예를 들어, 고양이와 개를 구분할 때 고양이와 개를 나누는 선이 Decision Boundary임. 이 경계에 따라 모델이 데이터를 분류하게 됨.    \n",
    "\n",
    "5. Feature Space(특징 공간):    \n",
    "\n",
    "설명: Feature Space는 데이터를 표현하는 공간임. 예를 들어, 우리가 여러 가지 특징(키, 나이, 몸무게 등)을 가지고 사람을 구분할 때, 이 모든 특징을 표현하는 공간을 Feature Space라고 부름. 이 공간 안에서 데이터들이 어떻게 분포되어 있는지가 중요함.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Saturation(기울기 포화): 신경망에서 기울기가 거의 0이 되어 학습이 멈추는 상태.     \n",
    "SGD 학습(Stochastic Gradient Descent): 데이터를 작은 덩어리로 나누어 반복적으로 학습시키는 방법.     \n",
    "Dead Neuron(죽은 뉴런): 항상 같은 값을 출력해 학습에 기여하지 못하는 뉴런.     \n",
    "Decision Boundary(결정 경계): 데이터를 다른 클래스들로 나누는 경계선.     \n",
    "Feature Space(특징 공간): 데이터의 여러 특징들을 표현하는 다차원 공간.   \n",
    "Loss : 모델이 얼마나 틀렸는가를 나타내는 지표.  \n",
    "Gradient : 함수가 가장 빠르게 변하는 방향과 그 변화의 크기.  \n",
    "Gradient Descent : 함수를 가장 빠르게 줄이기 위해 경사를 따라 내려가듯 조금씩 움직여서 Loss를 줄임.  \n",
    "Derivative : 미분을 통해 기울기를 구하는 과정. Loss를 줄이기 위해 가중치(W)를 어떻게 바꿀지 Partial   Differentiation(편미분)을 통해 찾음.  \n",
    "Global Minimum(전역 최소점): Global Minimum은 전체 함수에서 손실이 가장 작은 지점을 의미함.     \n",
    "Jacobian(야코비안):Jacobian은 다변수 함수의 각 변수에 대한 편미분값을 모아서 만든 행렬임.    \n",
    "Convexity(볼록성): Convexity는 함수가 오직 하나의 최저점(최소값)을 가지는 성질을 의미함.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Partial Differentiation(편미분):  \n",
    "\n",
    "특정 변수에 집중해서 그 변수에 대한 변화량(기울기)을 계산하는 방법임.  \n",
    "\n",
    "2) Chain Rule(연쇄 법칙):  \n",
    "\n",
    "여러 함수가 연결된 경우, 각 함수의 미분을 연쇄적으로 곱해서 전체 함수를 미분하는 방법임. 딥러닝의 역전파(Backward Propagation)에서 매우 중요함.  \n",
    "\n",
    "3) Auto Differentiation(자동 미분)과 Jacobian(야코비안):  \n",
    "\n",
    "컴퓨터가 미분을 효율적으로 계산하는 방법임. 연쇄 법칙(Chain Rule)을 사용해서 각 가중치(weight)에 대한 손실의 변화를 효율적으로 계산함.  \n",
    "\n",
    "4) 경사 하강(Gradient Descent)이 잘 작동하기 위한 조건:  \n",
    "\n",
    "경사 하강을 통해 손실을 줄이려면 함수가 Convex(볼록)해야 함. 그래야 Global Minimum(전역 최소점)으로 쉽게   도달할 수 있음.\n",
    "Convex 함수는 오직 하나의 최저점(가장 낮은 지점)을 가지는 함수임. 경사 하강법은 이 점을 찾는 과정임.  \n",
    "Convex하지 않은 함수에서는 경사 하강이 Local Minimum(지역 최소점)에 갇힐 수 있음.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
