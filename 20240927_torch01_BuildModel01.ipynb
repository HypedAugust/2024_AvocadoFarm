{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38-creating our first PyTorch model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예전 Linear Regression 코드 \n",
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 본격적인 데이터 가공 시작 \n",
    "# splitting data into training and test sets \n",
    "# one of the most important concepts in machine learning in general \n",
    "\n",
    "## lets create a training and test set with our data. \n",
    "# create known parameters \n",
    "weight = 0.7 \n",
    "bias = 0.3 # 원래 weight랑 bias 합쳐서 1로 만들어? 선형회귀는? \n",
    "\n",
    "# create\n",
    "start = 0 # start end step 은 왜 있는거야 무슨 의미야? arange 때문이야? 이거 그냥 임의값이지? \n",
    "end = 1 \n",
    "step = 0.02 \n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # unsqueeze 해서 1 차원 줄이는거지? \n",
    "y = weight*X + bias  \n",
    "\n",
    "\n",
    "X[:10], y[:10] \n",
    "len(X), len(y) #결과 50,50\n",
    "\n",
    "# create a train/test split \n",
    "train_split = int(0.8* len(X)) # 80을 트레이닝으로 가져갈거고 test를 20으로 가져갈 거임. \n",
    "X_train, y_train = X[:train_split], y[:train_split] # y의 처음부터 train_split까지의 값을 훈련용 출력값으로 가져와.\n",
    "X_test, y_test = X[train_split:], y[train_split:] # 나머지 20이 test여서 가져옴 \n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)\n",
    "# 각 데이터셋의 크기를 출력하는 코드야. 이 코드로 훈련 데이터와 테스트 데이터의 개수를 확인할 수 있어.\n",
    "\n",
    "# you must creat training set/ testing set but you do not always need validation set \n",
    "# lets use function to visualize it \n",
    "def plot_predictions(train_data=X_train, \n",
    "                     train_labels=y_train, \n",
    "                     test_data=X_test, \n",
    "                     test_labels=y_test, \n",
    "                     predictions=None):# we don't have any predictions yet \n",
    "    \n",
    "# **y_train**은 **훈련 데이터에 해당하는 정답(출력 값)**으로, \n",
    "# 입력 데이터에 대한 실제 레이블이야. 그래서 여기서는 **train_labels**라고 부른 거야.\n",
    "  \"\"\"\n",
    "  Plots training data, test data and compares predictions.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 7))\n",
    "\n",
    "  # Plot training data in blue\n",
    "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "  # c가 컬러여서. b는 blue 이다. s는 size\n",
    "  \n",
    "  # Plot test data in green\n",
    "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "# are there predictions? \n",
    "  if predictions is not None:\n",
    "    # Plot the predictions in red (predictions were made on the test data)\n",
    "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "  # Show the legend\n",
    "  # 데이터 시리즈나 요소들이 무엇을 의미하는지 설명하는 **범례(legend)**\n",
    "  plt.legend(prop={\"size\": 14});\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Linear Regression model class\n",
    "\n",
    "class LinearRegressionModel(nn.Module): # super해서 여기서 가져옴 \n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1,\n",
    "                                                requires_grad = True, # gradient descent \n",
    "                                                dtype = torch.float\n",
    "                                                )) \n",
    "        # 모델이 학습하는 파라미터(가중치 또는 편향)로 등록할 값을 지정하는 함수야.\n",
    "        self.bias = nn.Parameter(torch.randn(1,\n",
    "                                              requires_grad=True,\n",
    "                                              dtype = torch.float))\n",
    "        # w가 웨이트고 b가 intercept야. \n",
    "        \n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor: # ->함수 주석(Annotation) 방식\n",
    "            # 입력값도 텐서로 처리하고, 반환값도 텐서로 처리\n",
    "            return self.Weights*x+self.bias \n",
    "\n",
    "\n",
    "# checking the contents of our PyTorch model\n",
    "\n",
    "# create a random seed \n",
    "torch.manual_seed(42) \n",
    "# seed는 무작위성을 통제하는 값 \n",
    "# 랜덤 시드(random seed)**를 설정하면, 랜덤한 값들이 항상 동일하게 생성\n",
    "# 이 코드를 사용하면, 랜덤하게 생성되는 숫자들이 항상 동일하게 나와\n",
    "# 시드를 고정하면, 다른 사람과 같은 환경에서 동일한 결과를 재현할 수 있어.\n",
    "\n",
    "# Create an instance of the model \n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check out the parameters\n",
    "list(model_0.parameters())  \n",
    "# 렌덤시드의 값으로 모델의 가중치와 편향값을 부르는 코드 \n",
    "# 리스트 형태 : 한 번에 모아서 쉽게 확인\n",
    "# 리스트로 묶어서 확인하면 몇 개의 파라미터가 있는지 쉽게 볼 수 있음 \n",
    "# 모델의 모든 파라미터(가중치와 편향)를 반환\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List named parameters \n",
    "model_0.state_dict() \n",
    "# state_dict : 델의 모든 파라미터 값들(가중치, 편향 등)을 딕셔너리 형태로 반환\n",
    "# 모델을 저장하거나 불러올 때 모델의 상태(파라미터 값들)를 저장\n",
    "# 'weights': 가중치 값\n",
    "# 'bias': 편향 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# X_test의 모양 확인\n",
    "print(X_test.shape)\n",
    "\n",
    "# 만약 1차원이라면 2차원으로 변환\n",
    "if len(X_test.shape) == 1:\n",
    "    X_test = X_test.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch model building essentials  \n",
    "- torch.nn - contains all of the buildings for computational graphs (a neural network can be considered a computational graph)  \n",
    "- torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us  \n",
    "- torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()  \n",
    "- torch.optim - this where the optimizers in PyTorch live, they will help with gradient descent\n",
    "- def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're going to be building classes throughout the course, I'd recommend getting familiar with OOP in Python, to do so you can use the following resource from Real Python: https://realpython.com/python3-object-oriented-programming/\n",
    "\n",
    "What our model does:\n",
    "\n",
    "Start with random values (weight & bias)\n",
    "Look at training data and adjust the random values to better represent (or get closer to) the ideal values (the weight & bias values we used to create the data)\n",
    "How does it do so?\n",
    "\n",
    "Through two main algorithms:\n",
    "\n",
    "Gradient descent - https://youtu.be/IHZwWFHWa-w\n",
    "Backpropagation - https://youtu.be/Ilg3gGewQ5U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [LinearRegressionModel] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m y_preds\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# y-hat 값을 뽑음 \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# **X_test**는 모델이 학습하지 않은 테스트 데이터로, 일반적으로 **훈련 데이터(X_train)**\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 모델의 forward() 함수가 실행되고, X_test 값이 \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# **가중치(weights)**와 곱해진 후 **편향(bias)**가 더해져서 최종 **예측값(y-hat)**이 계산\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\qkim8\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch-new-94B1aYL8-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\qkim8\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch-new-94B1aYL8-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:363\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [LinearRegressionModel] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "\n",
    "y_preds = model_0(X_test)\n",
    "y_preds\n",
    "# y-hat 값을 뽑음 \n",
    "# **X_test**는 모델이 학습하지 않은 테스트 데이터로, 일반적으로 **훈련 데이터(X_train)**\n",
    "# 모델의 forward() 함수가 실행되고, X_test 값이 \n",
    "# **가중치(weights)**와 곱해진 후 **편향(bias)**가 더해져서 최종 **예측값(y-hat)**이 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 2 (1145169076.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[38], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    y_preds = model_0(X_test)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'with' statement on line 2\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with model\n",
    "with torch.inference_mode():\n",
    "  y_preds = model_0(X_test)\n",
    "  \n",
    "# You can also do something similar with torch.no_grad(), \n",
    "# however, torch.inference_mode() is preferred\n",
    "# with torch.no_grad(): y_preds = model_0(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-new-94B1aYL8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
